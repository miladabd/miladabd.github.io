<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Micah  Goldblum</title>
<meta name="description" content="Micah Goldblum. Math and ML.
">

<!-- Open Graph -->

<meta property="og:site_name" content="Micah Goldblum. Math and ML.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="/" />
<meta property="og:description" content="about" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%67%6F%6C%64%62%6C%75%6D@%75%6D%64.%65%64%75"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=pGDKzuUAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/goldblum" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/micahgoldblum" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          <!-- CV -->
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/cv.pdf">
                CV
                 
            </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
          

        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Micah</span>  Goldblum
    </h1>
     <p class="desc">Math and ML.  Trying to understand neural networks better.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am currently a postdoctoral research fellow at the University of Maryland working with <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a>.  My broad research focus lies at the intersection of machine learning and optimization with the aim of creating practical systems that are secure, efficient, and reliable on real-world problems.  Current areas of study range from dataset security to few-shot learning to making neural networks that can think deeper to extrapolate their knowledge to solve harder problems.  Before my current position, I received a Ph.D. in mathematics at the University of Maryland.</p>


    </div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/meta.png">
  
  </div>

  <div id="goldblum2020adversarially1" class="col-sm-8">
    
      <div class="title">Adversarially Robust Few-Shot Learning: A Meta-Learning Approach</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems (NeurIPS),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1910.00982" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, few-shot learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/variance_2.png">
  
  </div>

  <div id="goldblum2020unraveling" class="col-sm-8">
    
      <div class="title">Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Reich, Steven,
                
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Ni, Renkun,
                
              
            
          
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning (ICML),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2002.06753" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we introduce and verify several hypotheses for why meta-learned models perform better. Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification. In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/propaganda.png">
  
  </div>

  <div id="goldblum2019truth" class="col-sm-8">
    
      <div class="title">Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Moeller, Michael,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations (ICLR),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1910.00359" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/distilled.png">
  
  </div>

  <div id="goldblum2020adversarially2" class="col-sm-8">
    
      <div class="title">Adversarially Robust Distillation</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Feizi, Soheil,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1905.09747" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/cutmix.jpeg">
  
  </div>

  <div id="ni2021data" class="col-sm-8">
    
      <div class="title">Data Augmentation for Meta-Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  Ni, Renkun,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Sharaf, Amr,
                
              
            
          
        
          
          
          
            
              
                
                  Kong, Kezhi,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Machine Learning (ICML),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2010.07092" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, practitioners use sophisticated data augmentation schemes to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample support data, query data, and tasks on each training step. In this complex sampling scenario, data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes/tasks. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/data-poisoning.jpeg">
  
  </div>

  <div id="schwarzschild2020just" class="col-sm-8">
    
      <div class="title">Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Gupta, Arjun,
                
              
            
          
        
          
          
          
            
              
                
                  Dickerson, John P,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Machine Learning (ICML),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2006.12557" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/lowkey.png">
  
  </div>

  <div id="cherepanova2021lowkey" class="col-sm-8">
    
      <div class="title">LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition</div>
      <div class="author">
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Foley, Harrison,
                
              
            
          
        
          
          
          
            
              
                
                  Duan, Shiyuan,
                
              
            
          
        
          
          
          
            
              
                
                  Dickerson, John P,
                
              
            
          
        
          
          
          
            
              
                
                  Taylor, Gavin,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations (ICLR),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2101.07922" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike. These systems are typically built by scraping social media profiles for user images. Adversarial perturbations have been proposed for bypassing facial recognition systems. However, existing methods fail on full-scale systems and commercial APIs. We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases. Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/manifold.png">
  
  </div>

  <div id="pope2021intrinsic" class="col-sm-8">
    
      <div class="title">The Intrinsic Dimension of Images and Its Impact on Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  Pope, Phillip,
                
              
            
          
        
          
          
          
            
              
                
                  Zhu, Chen,
                
              
            
          
        
          
          
          
            
              
                
                  Abdelkader, Ahmed,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations (ICLR),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2104.08894" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%67%6F%6C%64%62%6C%75%6D@%75%6D%64.%65%64%75"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=pGDKzuUAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/goldblum" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/micahgoldblum" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Micah  Goldblum.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
