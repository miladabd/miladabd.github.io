<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Micah  Goldblum | publications</title>
<meta name="description" content="Micah Goldblum. Math and ML.
">

<!-- Open Graph -->

<meta property="og:site_name" content="Micah Goldblum. Math and ML.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="/publications/" />
<meta property="og:description" content="publications" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Micah</span>   Goldblum
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          <!-- CV -->
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/cv.pdf">
                CV
                 
            </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
          

        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/cutmix.jpeg" />
  
  </div>

  <div id="ni2021data" class="col-sm-8">
    
      <div class="title">Data Augmentation for Meta-Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  Ni, Renkun,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Sharaf, Amr,
                
              
            
          
        
          
          
          
            
              
                
                  Kong, Kezhi,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Machine Learning (ICML),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2010.07092" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, practitioners use sophisticated data augmentation schemes to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample support data, query data, and tasks on each training step. In this complex sampling scenario, data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes/tasks. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/data-poisoning.jpeg" />
  
  </div>

  <div id="schwarzschild2020just" class="col-sm-8">
    
      <div class="title">Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Gupta, Arjun,
                
              
            
          
        
          
          
          
            
              
                
                  Dickerson, John P,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Machine Learning (ICML),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2006.12557" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/lowkey.png" />
  
  </div>

  <div id="cherepanova2021lowkey" class="col-sm-8">
    
      <div class="title">LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition</div>
      <div class="author">
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Foley, Harrison,
                
              
            
          
        
          
          
          
            
              
                
                  Duan, Shiyuan,
                
              
            
          
        
          
          
          
            
              
                
                  Dickerson, John P,
                
              
            
          
        
          
          
          
            
              
                
                  Taylor, Gavin,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations (ICLR),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2101.07922" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike. These systems are typically built by scraping social media profiles for user images. Adversarial perturbations have been proposed for bypassing facial recognition systems. However, existing methods fail on full-scale systems and commercial APIs. We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases. Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="goldblum2021dataset" class="col-sm-8">
    
      <div class="title">Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Tsipras, Dimitris,
                
              
            
          
        
          
          
          
            
              
                
                  Xie, Chulin,
                
              
            
          
        
          
          
          
            
              
                
                  Chen, Xinyun,
                
              
            
          
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Song, Dawn,
                
              
            
          
        
          
          
          
            
              
                
                  Madry, Aleksander,
                
              
            
          
        
          
          
          
            
              
                
                  Li, Bo,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2012.10544" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/manifold.png" />
  
  </div>

  <div id="pope2021intrinsic" class="col-sm-8">
    
      <div class="title">The Intrinsic Dimension of Images and Its Impact on Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  Pope, Phillip,
                
              
            
          
        
          
          
          
            
              
                
                  Zhu, Chen,
                
              
            
          
        
          
          
          
            
              
                
                  Abdelkader, Ahmed,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations (ICLR),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2104.08894" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="borgnia2021strong" class="col-sm-8">
    
      <div class="title">Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff</div>
      <div class="author">
        
          
          
          
            
              
                
                  Borgnia, Eitan,
                
              
            
          
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Ghiasi, Amin,
                
              
            
          
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Goldstein, Tom,
                
              
            
          
        
          
          
          
            
              
                
                  and Gupta, Arjun
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2011.09527" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9%.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/meta.png" />
  
  </div>

  <div id="goldblum2020adversarially1" class="col-sm-8">
    
      <div class="title">Adversarially Robust Few-Shot Learning: A Meta-Learning Approach</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems (NeurIPS),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1910.00982" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, few-shot learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/variance_2.png" />
  
  </div>

  <div id="goldblum2020unraveling" class="col-sm-8">
    
      <div class="title">Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Reich, Steven,
                
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Ni, Renkun,
                
              
            
          
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning (ICML),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2002.06753" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we introduce and verify several hypotheses for why meta-learned models perform better. Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification. In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/propaganda.png" />
  
  </div>

  <div id="goldblum2019truth" class="col-sm-8">
    
      <div class="title">Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Moeller, Michael,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations (ICLR),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1910.00359" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/distilled.png" />
  
  </div>

  <div id="goldblum2020adversarially2" class="col-sm-8">
    
      <div class="title">Adversarially Robust Distillation</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Feizi, Soheil,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1905.09747" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="tran2020open" class="col-sm-8">
    
      <div class="title">An Open Review of OpenReview: A Critical Analysis of the Machine Learning Conference Review Process</div>
      <div class="author">
        
          
          
          
            
              
                
                  Tran, David,
                
              
            
          
        
          
          
          
            
              
                
                  Valtchanov, Alex,
                
              
            
          
        
          
          
          
            
              
                
                  Ganapathy, Keshav,
                
              
            
          
        
          
          
          
            
              
                
                  Feng, Raymond,
                
              
            
          
        
          
          
          
            
              
                
                  Slud, Eric,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2010.05137" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Mainstream machine learning conferences have seen a dramatic increase in the number of participants, along with a growing range of perspectives, in recent years. Members of the machine learning community are likely to overhear allegations ranging from randomness of acceptance decisions to institutional bias. In this work, we critically analyze the review process through a comprehensive study of papers submitted to ICLR between 2017 and 2020. We quantify reproducibility/randomness in review scores and acceptance decisions, and examine whether scores correlate with paper impact. Our findings suggest strong institutional bias in accept/reject decisions, even after controlling for paper quality. Furthermore, we find evidence for a gender gap, with female authors receiving lower scores, lower acceptance rates, and fewer citations per paper than their male counterparts. We conclude our work with recommendations for future conference organizers.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="chiang2020witchcraft" class="col-sm-8">
    
      <div class="title">Witchcraft: Efficient PGD Attacks with Random Step Size</div>
      <div class="author">
        
          
          
          
            
              
                
                  Chiang, Ping-Yeh,
                
              
            
          
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Goldstein, Tom,
                
              
            
          
        
          
          
          
            
              
                
                  Ni, Renkun,
                
              
            
          
        
          
          
          
            
              
                
                  Reich, Steven,
                
              
            
          
        
          
          
          
            
              
                
                  and Shafahi, Ali
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1911.07989" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="goldblum2019sheared" class="col-sm-8">
    
      <div class="title">Sheared Multi-Scale Weight Sharing for Multi-Spectral Superresolution</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  and Czaja, Wojciech
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Algorithms, Technologies, and Applications for Multispectral and Hyperspectral Imagery XXV,</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10986/109860X/Sheared-multi-scale-weight-sharing-for-multi-spectral-superresolution/10.1117/12.2519982.short?SSO=1" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep learning approaches to single-image superresolution typically use convolutional neural networks. Convolutional layers introduce translation invariance to neural networks. However, other spatial invariants appear in imaging data. Two such invariances are scale invariance, similar features at multiple spacial scales, and shearing invariance. We investigate these invariances by using weight sharing between dilated and sheared convolutional kernels in the context of multi-spectral imaging data. Traditional pooling methods can extract features at coarse spacial levels. Our approach explores a finer range of scales. Additionally, our approach offers improved storage efficiency because dilated and sheared convolutions allows single trainable kernels to extract information at multiple spacial scales and shears without the costs of training and storing many filters, especially in multi-spectral imaging where data representations are complex.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">preprint</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="goldblum2020adversarial" class="col-sm-8">
    
      <div class="title">Adversarial Attacks on Machine Learning Systems for High-Frequency Trading</div>
      <div class="author">
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Patel, Ankit B,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2002.09565" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Algorithmic trading systems are often completely automated, and deep learning is increasingly receiving attention in this domain. Nonetheless, little is known about the robustness properties of these models. We study valuation models for algorithmic trading from the perspective of adversarial machine learning. We introduce new attacks specific to this domain with size constraints that minimize attack costs. We further discuss how these attacks can be used as an analysis tool to study and evaluate the robustness properties of financial models. Finally, we investigate the feasibility of realistic adversarial attacks in which an adversarial trader fools automated trading systems into making inaccurate predictions.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="somepalli2021saint" class="col-sm-8">
    
      <div class="title">SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training</div>
      <div class="author">
        
          
          
          
            
              
                
                  Somepalli, Gowthami,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Bruss, C Bayan,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.01342" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners. However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems. Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="fowl2021adversarial" class="col-sm-8">
    
      <div class="title">Adversarial Examples Make Strong Poisons</div>
      <div class="author">
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Chiang, Ping-yeh,
                
              
            
          
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                
                  Czaja, Wojtek,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.10807" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data. In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. Our findings indicate that adversarial examples, when assigned the original label of their natural base image, cannot be used to train a classifier for natural images. Furthermore, when adversarial examples are assigned their adversarial class label, they are useful for training. This suggests that adversarial examples contain useful semantic content, just with the “wrong” labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="bansal2021metabalance" class="col-sm-8">
    
      <div class="title">MetaBalance: High-Performance Neural Networks for Class-Imbalanced Data</div>
      <div class="author">
        
          
          
          
            
              
                
                  Bansal, Arpit,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Bruss, C Bayan,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.09643" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Class-imbalanced data, in which some classes contain far more samples than others, is ubiquitous in real-world applications. Standard techniques for handling class-imbalance usually work by training on a re-weighted loss or on re-balanced data. Unfortunately, training overparameterized neural networks on such objectives causes rapid memorization of minority class data. To avoid this trap, we harness meta-learning, which uses both an ”outer-loop” and an ”inner-loop” loss, each of which may be balanced using different strategies. We evaluate our method, MetaBalance, on image classification, credit-card fraud detection, loan default prediction, and facial recognition tasks with severely imbalanced data, and we find that MetaBalance outperforms a wide array of popular re-sampling strategies.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="souri2021sleeper" class="col-sm-8">
    
      <div class="title">Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch</div>
      <div class="author">
        
          
          
          
            
              
                
                  Souri, Hossein,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Chellappa, Rama,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.08970" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As the curation of data for machine learning becomes increasingly automated, dataset tampering is a mounting threat. Backdoor attackers tamper with training data to embed a vulnerability in models that are trained on that data. This vulnerability is then activated at inference time by placing a "trigger" into the model’s input. Typical backdoor attacks insert the trigger directly into the training data, although the presence of such an attack may be visible upon inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning without placing a trigger into the training data at all. However, this hidden trigger attack is ineffective at poisoning neural networks trained from scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs gradient matching, data selection, and target model re-training during the crafting process. Sleeper Agent is the first hidden trigger backdoor attack to be effective against neural networks trained from scratch. We demonstrate its effectiveness on ImageNet and in black-box settings.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="fowl2020random" class="col-sm-8">
    
      <div class="title">Random Network Distillation as a Diversity Metric for Both Image and Text Generation</div>
      <div class="author">
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Gupta, Arjun,
                
              
            
          
        
          
          
          
            
              
                
                  Sharaf, Amr,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2010.06715" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="ronny2019understanding" class="col-sm-8">
    
      <div class="title">Understanding Generalization through Visualizations</div>
      <div class="author">
        
          
          
          
            
              
                
                  Ronny Huang, W,
                
              
            
          
        
          
          
          
            
              
                
                  Emam, Zeyad,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Terry, Justin K,
                
              
            
          
        
          
          
          
            
              
                
                  Huang, Furong,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/1906.03291" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The power of neural networks lies in their ability to generalize to unseen data, yet the underlying reasons for this phenomenon remain elusive. Numerous rigorous attempts have been made to explain generalization, but available bounds are still quite loose, and analysis does not always lead to true understanding. The goal of this work is to make generalization more intuitive. Using visualization methods, we discuss the mystery of generalization, the geometry of loss landscapes, and how the curse (or, rather, the blessing) of dimensionality causes optimizers to settle into minima that generalize well.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="schwarzschild2021can" class="col-sm-8">
    
      <div class="title">Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Borgnia, Eitan,
                
              
            
          
        
          
          
          
            
              
                
                  Gupta, Arjun,
                
              
            
          
        
          
          
          
            
              
                
                  Huang, Furong,
                
              
            
          
        
          
          
          
            
              
                
                  Vishkin, Uzi,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.04537" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difficult for neural models. Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time. In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on prefix sum computation, mazes, and chess. In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by "thinking for longer."</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="schwarzschild2021uncanny" class="col-sm-8">
    
      <div class="title">The Uncanny Similarity of Recurrence and Depth</div>
      <div class="author">
        
          
          
          
            
              
                
                  Schwarzschild, Avi,
                
              
            
          
        
          
          
          
            
              
                
                  Gupta, Arjun,
                
              
            
          
        
          
          
          
            
              
                
                  Ghiasi, Amin,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2102.11011" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits as depth despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="fowl2021preventing" class="col-sm-8">
    
      <div class="title">Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release</div>
      <div class="author">
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Chiang, Ping-yeh,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                
                  Bansal, Arpit,
                
              
            
          
        
          
          
          
            
              
                
                  Czaja, Wojtek,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2103.02683" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large organizations such as social media companies continually release data, for example user images. At the same time, these organizations leverage their massive corpora of released data to train proprietary models that give them an edge over their competitors. These two behaviors can be in conflict as an organization wants to prevent competitors from using their own data to replicate the performance of their proprietary models. We solve this problem by developing a data poisoning method by which publicly released data can be minimally modified to prevent others from train-ing models on it. Moreover, our method can be used in an online fashion so that companies can protect their data in real time as they release it. We demonstrate the success of our approach on ImageNet classification and on facial recognition.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="shu2020prepare" class="col-sm-8">
    
      <div class="title">Prepare for the Worst: Generalizing across Domain Shifts with Adversarial Batch Normalization</div>
      <div class="author">
        
          
          
          
            
              
                
                  Shu, Manli,
                
              
            
          
        
          
          
          
            
              
                
                  Wu, Zuxuan,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2009.08965" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Adversarial training is the industry standard for producing models that are robust to small adversarial perturbations. However, machine learning practitioners need models that are robust to domain shifts that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by adversarially perturbing these feature statistics, rather than image pixels, to produce models that are robust to domain shift. We also visualize images from adversarially crafted distributions. Our method, Adversarial Batch Normalization (AdvBN), significantly improves the performance of ResNet-50 on ImageNet-C (+8.1%), Stylized-ImageNet (+6.7%), and ImageNet-Instagram (+3.9%) over standard training practices. In addition, we demonstrate that AdvBN can also improve generalization on semantic segmentation.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="geiping2021doesn" class="col-sm-8">
    
      <div class="title">What Doesn’t Kill You Makes You Robust (er): Adversarial Training against Poisons and Backdoors</div>
      <div class="author">
        
          
          
          
            
              
                
                  Geiping, Jonas,
                
              
            
          
        
          
          
          
            
              
                
                  Fowl, Liam,
                
              
            
          
        
          
          
          
            
              
                
                  Somepalli, Gowthami,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Moeller, Michael,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2102.13624" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data poisoning is a threat model in which a malicious actor tampers with training data to manipulate outcomes at inference time. A variety of defenses against this threat model have been proposed, but each suffers from at least one of the following flaws: they are easily overcome by adaptive attacks, they severely reduce testing performance, or they cannot generalize to diverse data poisoning threat models. Adversarial training, and its variants, is currently considered the only empirically strong defense against (inference-time) adversarial attacks. In this work, we extend the adversarial training framework to instead defend against (training-time) poisoning and backdoor attacks. Our method desensitizes networks to the effects of poisoning by creating poisons during training and injecting them into training batches. We show that this defense withstands adaptive attacks, generalizes to diverse threat models, and incurs a better performance trade-off than previous defenses.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
  </div>

  <div id="cherepanova2021technical" class="col-sm-8">
    
      <div class="title">Technical Challenges for Training Fair Neural Networks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Cherepanova, Valeriia,
                
              
            
          
        
          
          
          
            
              
                
                  Nanda, Vedant,
                
              
            
          
        
          
          
          
            
              
                <em>Goldblum, Micah</em>,
              
            
          
        
          
          
          
            
              
                
                  Dickerson, John P,
                
              
            
          
        
          
          
          
            
              
                
                  and Goldstein, Tom
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        preprint
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2102.06764" class="btn btn-sm z-depth-0" role="button" target="_blank">LINK</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As machine learning algorithms have been widely deployed across applications, many concerns have been raised over the fairness of their predictions, especially in high stakes settings (such as facial recognition and medical imaging). To respond to these concerns, the community has proposed and formalized various notions of fairness as well as methods for rectifying unfair behavior. While fairness constraints have been studied extensively for classical models, the effectiveness of methods for imposing fairness on deep neural networks is unclear. In this paper, we observe that these large models overfit to fairness objectives, and produce a range of unintended and undesirable consequences. We conduct our experiments on both facial recognition and automated medical diagnosis datasets using state-of-the-art architectures.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Micah  Goldblum.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
